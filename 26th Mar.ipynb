{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. **Simple Linear Regression vs. Multiple Linear Regression:**\n",
    "   - **Simple Linear Regression:** In simple linear regression, there is a single independent variable (predictor) that is used to predict a continuous dependent variable (response). The relationship between the variables is represented by a straight line equation, typically in the form of Y = a + bX, where Y is the dependent variable, X is the independent variable, \"a\" is the intercept, and \"b\" is the slope.\n",
    "   - **Example:** Predicting a student's final exam score (Y) based on the number of hours spent studying (X). Here, X is the single independent variable.\n",
    "\n",
    "   - **Multiple Linear Regression:** In multiple linear regression, there are multiple independent variables used to predict a continuous dependent variable. The relationship is represented by an equation like Y = a + b1X1 + b2X2 + ... + bnXn, where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, \"a\" is the intercept, and b1, b2, ..., bn are the slopes associated with each independent variable.\n",
    "   - **Example:** Predicting a house's sale price (Y) based on factors like square footage (X1), number of bedrooms (X2), and distance from the city center (X3). Here, X1, X2, and X3 are multiple independent variables.\n",
    "\n",
    "Q2. **Assumptions of Linear Regression:**\n",
    "   - Linearity: The relationship between the independent and dependent variables is linear.\n",
    "   - Independence: The residuals (the differences between observed and predicted values) are independent of each other.\n",
    "   - Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables.\n",
    "   - Normality: The residuals follow a normal distribution.\n",
    "   \n",
    "   To check these assumptions, you can use diagnostic plots (like residual plots), statistical tests (e.g., Shapiro-Wilk test for normality), and examining scatterplots for linearity and homoscedasticity.\n",
    "\n",
    "Q3. **Interpretation of Slope and Intercept:**\n",
    "   - **Slope (b):** It represents the change in the dependent variable for a one-unit change in the independent variable while keeping other variables constant.\n",
    "   - **Intercept (a):** It represents the predicted value of the dependent variable when all independent variables are set to zero.\n",
    "\n",
    "   Example: In a linear regression model predicting salary (Y) based on years of experience (X), the slope (b) represents the change in salary for each additional year of experience, and the intercept (a) represents the expected salary for someone with zero years of experience.\n",
    "\n",
    "Q4. **Gradient Descent:**\n",
    "   - Gradient descent is an optimization algorithm used in machine learning to minimize the loss or error of a model. It works by iteratively adjusting model parameters (weights and biases) in the direction of steepest descent of the cost function. The goal is to find the parameter values that minimize the cost function, typically associated with training a machine learning model.\n",
    "   - In each iteration, gradient descent computes the gradient of the cost function with respect to the parameters and updates the parameters by taking a step proportional to the negative gradient.\n",
    "\n",
    "Q5. **Multiple Linear Regression Model:**\n",
    "   - Multiple linear regression is a statistical model that extends simple linear regression to incorporate multiple independent variables to predict a dependent variable. The model is represented as Y = a + b1X1 + b2X2 + ... + bnXn, where Y is the dependent variable, X1, X2, ..., Xn are the independent variables, \"a\" is the intercept, and b1, b2, ..., bn are the slopes for each independent variable.\n",
    "   - It differs from simple linear regression by considering the combined effect of multiple predictors on the dependent variable.\n",
    "\n",
    "Q6. **Multicollinearity in Multiple Linear Regression:**\n",
    "   - Multicollinearity occurs when independent variables in a multiple linear regression model are highly correlated with each other, making it challenging to separate their individual effects on the dependent variable.\n",
    "   - You can detect multicollinearity using correlation matrices, variance inflation factors (VIFs), or condition indices. To address it, you can remove one of the correlated variables, combine them, or use dimensionality reduction techniques like principal component analysis (PCA).\n",
    "\n",
    "Q7. **Polynomial Regression Model:**\n",
    "   - Polynomial regression is an extension of linear regression that allows for curved relationships between the independent and dependent variables. It uses polynomial terms (e.g., X^2, X^3) in addition to linear terms. The model equation is of the form Y = a + b1X + b2X^2 + ... + bnx^n.\n",
    "   - It differs from linear regression in that it can capture nonlinear patterns in the data.\n",
    "\n",
    "Q8. **Advantages and Disadvantages of Polynomial Regression:**\n",
    "   - **Advantages:**\n",
    "     - Captures nonlinear relationships in data.\n",
    "     - Flexible and can fit complex patterns.\n",
    "     - Can be a good choice when a linear model is too simplistic.\n",
    "\n",
    "   - **Disadvantages:**\n",
    "     - Prone to overfitting with higher-degree polynomials.\n",
    "     - Interpretability decreases with higher-degree polynomials.\n",
    "     - Sensitive to outliers.\n",
    "   \n",
    "   Polynomial regression is preferred when there is evidence of a nonlinear relationship between variables, but it should be used judiciously to avoid overfitting. Linear regression is simpler and may be preferred when the relationship is predominantly linear."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
