{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that an employee is a smoker given that they use the health insurance plan is: 0.40\n"
     ]
    }
   ],
   "source": [
    "# Q1 -->\n",
    "# Given probabilities\n",
    "P_A = 0.70  # Probability that an employee uses the health insurance plan\n",
    "P_B_given_A = 0.40  # Probability that an employee is a smoker given that they use the health insurance plan\n",
    "\n",
    "# Calculate the probability that an employee is a smoker given that they use the health insurance plan\n",
    "P_B_and_A = P_B_given_A * P_A\n",
    "P_B_given_A = P_B_and_A / P_A\n",
    "\n",
    "print(f\"The probability that an employee is a smoker given that they use the health insurance plan is: {P_B_given_A:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Bernoulli Naive Bayes vs. Multinomial Naive Bayes:\n",
    "\n",
    "- **Bernoulli Naive Bayes:**\n",
    "  - For binary data (0 or 1).\n",
    "  - Used in tasks like document classification and spam filtering.\n",
    "  - Assumes binary features.\n",
    "\n",
    "- **Multinomial Naive Bayes:**\n",
    "  - For discrete data, often counts or frequencies.\n",
    "  - Commonly used in text classification and topic modeling.\n",
    "  - Assumes integer-valued features.\n",
    "\n",
    "### Q3. Handling Missing Values in Bernoulli Naive Bayes:\n",
    "\n",
    "- In Bernoulli Naive Bayes, missing values are ignored during training and prediction.\n",
    "- The model assumes that missing features don't contribute to the likelihood calculation.\n",
    "\n",
    "### Q4. Gaussian Naive Bayes for Multi-Class Classification:\n",
    "\n",
    "- Yes, Gaussian Naive Bayes can handle multi-class classification.\n",
    "- It assumes continuous features following a Gaussian distribution.\n",
    "- Each class has its mean and variance parameters.\n",
    "- During prediction, assigns the instance to the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4601, 57), (4601, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes:\n",
      "Accuracy: 0.8806\n",
      "Precision: 0.9070\n",
      "Recall: 0.8000\n",
      "F1 Score: 0.8501\n",
      "\n",
      "Multinomial Naive Bayes:\n",
      "Accuracy: 0.7861\n",
      "Precision: 0.7644\n",
      "Recall: 0.7154\n",
      "F1 Score: 0.7391\n",
      "\n",
      "Gaussian Naive Bayes:\n",
      "Accuracy: 0.8208\n",
      "Precision: 0.7193\n",
      "Recall: 0.9462\n",
      "F1 Score: 0.8173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Train classifiers\n",
    "bernoulli_nb.fit(X_train, y_train)\n",
    "multinomial_nb.fit(X_train, y_train)\n",
    "gaussian_nb.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_b = bernoulli_nb.predict(X_test)\n",
    "y_pred_m = multinomial_nb.predict(X_test)\n",
    "y_pred_g = gaussian_nb.predict(X_test)\n",
    "\n",
    "# Evaluate classifiers\n",
    "accuracy_b = accuracy_score(y_test, y_pred_b)\n",
    "precision_b = precision_score(y_test, y_pred_b)\n",
    "recall_b = recall_score(y_test, y_pred_b)\n",
    "f1_b = f1_score(y_test, y_pred_b)\n",
    "\n",
    "accuracy_m = accuracy_score(y_test, y_pred_m)\n",
    "precision_m = precision_score(y_test, y_pred_m)\n",
    "recall_m = recall_score(y_test, y_pred_m)\n",
    "f1_m = f1_score(y_test, y_pred_m)\n",
    "\n",
    "accuracy_g = accuracy_score(y_test, y_pred_g)\n",
    "precision_g = precision_score(y_test, y_pred_g)\n",
    "recall_g = recall_score(y_test, y_pred_g)\n",
    "f1_g = f1_score(y_test, y_pred_g)\n",
    "\n",
    "# Display results\n",
    "print(\"Bernoulli Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_b:.4f}\")\n",
    "print(f\"Precision: {precision_b:.4f}\")\n",
    "print(f\"Recall: {recall_b:.4f}\")\n",
    "print(f\"F1 Score: {f1_b:.4f}\\n\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_m:.4f}\")\n",
    "print(f\"Precision: {precision_m:.4f}\")\n",
    "print(f\"Recall: {recall_m:.4f}\")\n",
    "print(f\"F1 Score: {f1_m:.4f}\\n\")\n",
    "\n",
    "print(\"Gaussian Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_g:.4f}\")\n",
    "print(f\"Precision: {precision_g:.4f}\")\n",
    "print(f\"Recall: {recall_g:.4f}\")\n",
    "print(f\"F1 Score: {f1_g:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "1. **Bernoulli Naive Bayes:**\n",
    "   - **Accuracy:** 88.06%\n",
    "   - **Precision:** 90.70%\n",
    "   - **Recall:** 80.00%\n",
    "   - **F1 Score:** 85.01%\n",
    "   - **Observation:** Bernoulli Naive Bayes performs well across all metrics. It has high precision and accuracy, making it effective in correctly identifying spam emails while minimizing false positives.\n",
    "\n",
    "2. **Multinomial Naive Bayes:**\n",
    "   - **Accuracy:** 78.61%\n",
    "   - **Precision:** 76.44%\n",
    "   - **Recall:** 71.54%\n",
    "   - **F1 Score:** 73.91%\n",
    "   - **Observation:** Multinomial Naive Bayes shows decent performance but falls short compared to Bernoulli Naive Bayes. It may not capture binary features as effectively as Bernoulli NB, resulting in slightly lower precision and recall.\n",
    "\n",
    "3. **Gaussian Naive Bayes:**\n",
    "   - **Accuracy:** 82.08%\n",
    "   - **Precision:** 71.93%\n",
    "   - **Recall:** 94.62%\n",
    "   - **F1 Score:** 81.73%\n",
    "   - **Observation:** Gaussian Naive Bayes performs well in recall, indicating its ability to identify a high proportion of spam emails. However, the lower precision suggests a higher rate of false positives, impacting overall accuracy.\n",
    "\n",
    "### Overall Observations:\n",
    "\n",
    "- **Bernoulli Naive Bayes** appears to be the most balanced and effective in this context, providing a good trade-off between precision and recall. It is well-suited for binary features, making it suitable for spam classification tasks.\n",
    "\n",
    "- **Multinomial Naive Bayes** performs reasonably well but lags behind Bernoulli NB in accuracy and precision. It might be more suitable for tasks involving discrete count data.\n",
    "\n",
    "- **Gaussian Naive Bayes** excels in recall, indicating its strength in capturing spam instances, but it comes at the cost of precision. It may benefit from further tuning or feature engineering.\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- Naive Bayes assumes feature independence, which may not always hold in real-world scenarios.\n",
    "- The dataset size and characteristics may impact performance; a larger dataset could lead to more robust models.\n",
    "- The choice of Naive Bayes variant depends on the nature of the features in the dataset.\n",
    "\n",
    "### Future Work:\n",
    "\n",
    "- Experiment with feature engineering to enhance the performance of Gaussian Naive Bayes.\n",
    "- Explore additional hyperparameter tuning to optimize the performance of each classifier.\n",
    "- Consider using more sophisticated models or ensemble methods to further improve classification accuracy.\n",
    "\n",
    "In summary, while each Naive Bayes variant has its strengths and weaknesses, Bernoulli Naive Bayes is the most balanced for this spam classification task based on the provided results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
