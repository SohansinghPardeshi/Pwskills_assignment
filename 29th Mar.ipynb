{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. **Lasso Regression and Its Differences**:\n",
    "   - Lasso Regression is a linear regression technique that adds a penalty term (usually denoted as \"lambda\" or \"alpha\") to the ordinary least squares (OLS) cost function. This penalty term encourages the model to have sparse coefficients, effectively setting some of them to exactly zero.\n",
    "   - The main difference from other regression techniques, such as Ridge Regression, is that Lasso can perform feature selection by eliminating irrelevant features from the model. This is especially useful when dealing with high-dimensional datasets.\n",
    "\n",
    "Q2. **Advantage of Lasso Regression in Feature Selection**:\n",
    "   - The primary advantage of Lasso Regression is its ability to automatically select a subset of the most relevant features by setting the coefficients of less important features to zero. This helps simplify the model, reduce overfitting, and improve model interpretability.\n",
    "\n",
    "Q3. **Interpreting Lasso Regression Coefficients**:\n",
    "   - In Lasso Regression, non-zero coefficients represent features that are considered important for predicting the target variable.\n",
    "   - A coefficient of zero means that the corresponding feature has been effectively removed from the model.\n",
    "   - The sign and magnitude of non-zero coefficients indicate the direction and strength of the relationship between the feature and the target variable, similar to ordinary linear regression.\n",
    "\n",
    "Q4. **Tuning Parameters in Lasso Regression**:\n",
    "   - The primary tuning parameter in Lasso Regression is \"lambda\" (α or λ), which controls the strength of the regularization.\n",
    "   - A smaller lambda allows the model to fit the training data more closely and may lead to overfitting.\n",
    "   - A larger lambda results in stronger regularization, which encourages coefficient shrinkage and feature selection.\n",
    "   - The optimal lambda is typically chosen through techniques like cross-validation.\n",
    "\n",
    "Q5. **Lasso Regression for Non-Linear Problems**:\n",
    "   - Lasso Regression is inherently a linear method, but it can be used in combination with non-linear transformations of the input features to handle non-linear relationships. For example, you can include polynomial or interaction terms in your model to capture non-linearities, and Lasso will still perform feature selection among these transformed features.\n",
    "\n",
    "Q6. **Difference Between Ridge and Lasso Regression**:\n",
    "   - Ridge Regression adds a penalty term to prevent large coefficient values and reduce multicollinearity, but it doesn't set coefficients exactly to zero. Lasso Regression, on the other hand, can eliminate some features by setting their coefficients to zero, performing feature selection.\n",
    "   - Ridge tends to shrink coefficients towards zero without making them exactly zero, while Lasso can yield sparse models.\n",
    "\n",
    "Q7. **Handling Multicollinearity in Lasso Regression**:\n",
    "   - Lasso Regression is effective at handling multicollinearity by selecting a subset of relevant features and reducing the coefficients of less important correlated features to zero. This simplifies the model while preserving the most critical variables.\n",
    "\n",
    "Q8. **Choosing the Optimal Lambda Value in Lasso Regression**:\n",
    "   - The optimal lambda value in Lasso Regression is typically determined through techniques like cross-validation. You evaluate the model's performance with different lambda values and select the one that results in the best trade-off between model fit and feature selection. Cross-validation helps you find the lambda that generalizes well to new data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
