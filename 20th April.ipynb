{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1. What is the KNN algorithm?**\n",
    "- The k-nearest neighbors (KNN) algorithm is a simple and versatile supervised learning algorithm used for both classification and regression tasks. It operates on the principle of proximity, where the prediction for a new data point is based on the majority class (for classification) or the average (for regression) of its k-nearest neighbors in the feature space.\n",
    "\n",
    "**Q2. How do you choose the value of K in KNN?**\n",
    "- The choice of the value of K in KNN is crucial and depends on the characteristics of the data. A small K can make the model sensitive to noise, while a large K can make the model too influenced by the overall distribution. Common methods to choose K include cross-validation, grid search, and experimentation based on the nature of the problem.\n",
    "\n",
    "**Q3. What is the difference between KNN classifier and KNN regressor?**\n",
    "- In KNN classification, the algorithm predicts the class label of a new data point based on the majority class among its k-nearest neighbors. In KNN regression, the algorithm predicts a continuous value by averaging the target values of its k-nearest neighbors.\n",
    "\n",
    "**Q4. How do you measure the performance of KNN?**\n",
    "- Common performance metrics for KNN include accuracy, precision, recall, F1 score for classification tasks, and metrics like mean squared error or R-squared for regression tasks. Cross-validation is often used to get a more robust estimate of performance.\n",
    "\n",
    "**Q5. What is the curse of dimensionality in KNN?**\n",
    "- The curse of dimensionality in KNN refers to the phenomenon where the performance of the algorithm degrades as the number of features (dimensions) increases. In high-dimensional spaces, the distance between points becomes less meaningful, and the notion of \"closeness\" becomes less discriminative, leading to increased computational requirements and reduced performance.\n",
    "\n",
    "**Q6. How do you handle missing values in KNN?**\n",
    "- KNN can be sensitive to missing values. Imputation methods, such as replacing missing values with the mean or median, can be applied before using KNN. Another approach is to adapt the distance metric to handle missing values appropriately, or use algorithms that can handle missing values more robustly.\n",
    "\n",
    "**Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?**\n",
    "- KNN classifier is suitable for discrete classification problems, while KNN regressor is suitable for predicting continuous values in regression problems. The choice depends on the nature of the target variable in the problem.\n",
    "\n",
    "**Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?**\n",
    "- *Strengths:* Simplicity, no assumptions about the underlying data distribution.\n",
    "  *Weaknesses:* Sensitivity to irrelevant features, computational inefficiency for large datasets.\n",
    "  *Addressing weaknesses:* Feature selection, dimensionality reduction, and using optimized data structures (like KD-trees or ball trees) can improve efficiency.\n",
    "\n",
    "**Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?**\n",
    "- Euclidean distance is the straight-line distance between two points in the Euclidean space. Manhattan distance (or L1 distance) is the sum of the absolute differences between the coordinates of two points along each dimension. Both can be used as distance metrics in KNN, and the choice depends on the nature of the data.\n",
    "\n",
    "**Q10. What is the role of feature scaling in KNN?**\n",
    "- Feature scaling is essential in KNN because the algorithm relies on the distance between data points. If the features have different scales, the algorithm may be more influenced by features with larger magnitudes. Common methods include Min-Max scaling or Z-score normalization to bring features to a similar scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
