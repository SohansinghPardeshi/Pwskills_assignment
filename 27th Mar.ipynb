{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. **R-squared in Linear Regression**: R-squared (R²) is a statistical measure that represents the proportion of the variance in the dependent variable (target) that is explained by the independent variables (features) in a linear regression model. It ranges from 0 to 1, where 0 indicates that the model explains none of the variance, and 1 indicates that the model explains all of the variance. R-squared is calculated as the ratio of the explained variance to the total variance in the data.\n",
    "\n",
    "   Calculation: R² = 1 - (SSE / SST), where SSE is the sum of squared errors (residuals) and SST is the total sum of squares.\n",
    "\n",
    "Q2. **Adjusted R-squared**: Adjusted R-squared is a modified version of R-squared that accounts for the number of predictors (features) in the model. It penalizes the inclusion of unnecessary variables and provides a more realistic assessment of model fit. It increases only if the addition of a new predictor improves the model more than would be expected by chance.\n",
    "\n",
    "   Calculation: Adjusted R² = 1 - [(1 - R²) * (n - 1) / (n - k - 1)], where n is the number of observations and k is the number of predictors.\n",
    "\n",
    "Q3. **When to Use Adjusted R-squared**: Adjusted R-squared is more appropriate when you want to compare models with different numbers of predictors or when you suspect that some predictors might not be adding valuable information to the model. It helps in model selection by penalizing complexity.\n",
    "\n",
    "Q4. **RMSE, MSE, and MAE in Regression Analysis**:\n",
    "   - RMSE (Root Mean Squared Error): RMSE is a measure of the average deviation between predicted and actual values. It's calculated as the square root of the average of squared differences between predictions and actual values.\n",
    "   - MSE (Mean Squared Error): MSE is the average of squared differences between predicted and actual values.\n",
    "   - MAE (Mean Absolute Error): MAE is the average of absolute differences between predicted and actual values.\n",
    "\n",
    "   These metrics quantify the accuracy of predictions in regression analysis, with lower values indicating better performance.\n",
    "\n",
    "Q5. **Advantages and Disadvantages of RMSE, MSE, and MAE**:\n",
    "   - Advantages: These metrics are easy to interpret and widely used for regression evaluation. RMSE gives more weight to large errors.\n",
    "   - Disadvantages: They are sensitive to outliers. RMSE and MSE are influenced by large errors and may not reflect the overall model performance if outliers are present.\n",
    "\n",
    "Q6. **Lasso Regularization**: Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression to prevent overfitting by adding a penalty term to the linear regression cost function. Unlike Ridge regularization, Lasso encourages sparsity in the model by adding the absolute values of coefficients as a penalty term. It can be more appropriate when you suspect that only a subset of features is relevant, and it can perform feature selection.\n",
    "\n",
    "Q7. **Preventing Overfitting with Regularized Linear Models**: Regularized linear models like Lasso and Ridge add a penalty term to the linear regression cost function, which constrains the model's coefficients. This helps prevent overfitting by discouraging overly complex models. For example, in Lasso, some coefficients can become exactly zero, effectively removing irrelevant features from the model.\n",
    "\n",
    "Q8. **Limitations of Regularized Linear Models**: Regularized linear models assume a linear relationship between features and the target variable, which may not always hold. They also require tuning hyperparameters, and the choice of regularization method depends on the specific problem. Additionally, regularization may not be suitable when there's no prior knowledge about feature importance.\n",
    "\n",
    "Q9. **Comparing Models with Different Metrics**: The choice between RMSE and MAE depends on the problem and the specific goals. RMSE gives more weight to larger errors, so Model A with an RMSE of 10 might be preferred if large errors are more critical. However, Model B with an MAE of 8 might be chosen if you want to minimize the impact of extreme errors. The choice of metric should align with your project objectives and the nature of the errors in your dataset.\n",
    "\n",
    "Q10. **Comparing Regularized Models**: The choice between Ridge and Lasso regularization depends on the specific problem and the trade-offs between them. Model selection should involve cross-validation to assess performance. A smaller regularization parameter (e.g., 0.1) in Ridge may lead to less feature selection and more emphasis on fitting the data, while a larger Lasso parameter (e.g., 0.5) encourages sparsity and feature selection. The choice depends on the problem's requirements and the importance of feature selection."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
